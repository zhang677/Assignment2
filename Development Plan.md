- [x] Watch DQN videos
- [x] Revisit Pytorch training
- [x] Use random action to successfully run an episode
- [x] Collect the input 210x160x3 pictures to 84x84x1 using torchvision.transforms. Also need to get subwindow and cut out the other part
- [x] Use random action to successfully run an episode with a Wrapper
- [x] CHW (4,84,84) -> nn.Conv2d(4, 32, kernel_size=8, stride=4, bias=false) -> F.relu -> nn.Conv2d(32, 64, kernel_size=4, stride=2, bias=false) -> F.relu -> nn.Conv2d(64, 64, kernel_size=3, stride=1, bias=false) -> F.relu -> nn.Linear(3136,512) -> F.relu -> Linear(512,actions)
- [x] Figure out the output of the environment
- [x] Use Wrapper to load env(4), clip reward when traing and build another Wrapper with original reward when testing
- [ ] --Fix output figure and model saved--
- [x] Add epsilon scheduler
- [ ] --# of training frames and size of replay memory--
- [x] Add replay before traing (replay start size = 50000)
- [x] initialize the weights
- [x] Add eval (load from the current best eval_network)
- [x] Prepare evaluation env
- [x] Add main function argparse
- [x] Add Double learn
- [ ] Test the whole workflow
- [ ] Linear Q-network with experience replay and target fixing
- [ ] Performance plot across time for linear Q-network
- [ ] Table for linear Q-network
- [ ] Linear double Q-network
- [ ] Performance plot across time for linear double Q-network 
- [ ] Video on trained linear double Q-network 
- [ ] Table for linear double Q-network  
- [ ] Deep Q-network
- [ ] Performance plot across time for deep Q-network
- [ ] Video on trained deep Q-network 
- [ ] Table for deep Q-network
- [ ] Double deep Q-network
- [ ] Performance plot across time for double deep Q-network
- [ ] Video on trained deep Q-network
- [ ] Table for double deep Q-network
- [ ] Add the max operation in preprocess
- [ ] Add BatchNorm after Conv layer
